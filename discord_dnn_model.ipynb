{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "discord_dnn_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNyrSbiB4bgQ2DjuxOOFBG1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BaileyDalton007/discord_rnn/blob/main/discord_dnn_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XCxplkC1-E2Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec as w2v\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If needed upload w2v model\n",
        "model_save_file = files.upload()"
      ],
      "metadata": {
        "id": "C7wdAH_rBLVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload training csv's\n",
        "uploaded = files.upload()\n",
        "file_names = list(uploaded.keys())"
      ],
      "metadata": {
        "id": "cYCYdV5_E2VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load w2v model\n",
        "wv_model = w2v.load('word2vec.model')"
      ],
      "metadata": {
        "id": "ZV6cyDcPBerw"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Pre-processing**"
      ],
      "metadata": {
        "id": "ZBtgx_R3FSFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The uniform word count for each message\n",
        "# Shorter ones will be 0-padded and longer will be truncated\n",
        "WORD_COUNT = 20\n",
        "\n",
        "# Dimensionality of word vectors\n",
        "VECTOR_DIM = 100"
      ],
      "metadata": {
        "id": "m2qnsxqNDj73"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all csv's into one master dataframe\n",
        "frames_to_combine = []\n",
        "\n",
        "for file in file_names:\n",
        "  data = pd.read_csv(file, delimiter=',')\n",
        "  data.reset_index(drop=True)\n",
        "\n",
        "  frames_to_combine.append(data)\n",
        "\n",
        "# Combines all csv files into one dataframe to be processed\n",
        "master_df = pd.concat(frames_to_combine)\n",
        "master_df = master_df.reset_index(drop=True)\n",
        "\n",
        "# Tokenize training data\n",
        "d_set_arr = [master_df['Tmsg0'], master_df['Tmsg1'], master_df['Tmsg2'], master_df['Umsg']]\n",
        "\n",
        "tokenized_d_set = []\n",
        "\n",
        "for d_set in d_set_arr:\n",
        "  tmp = []\n",
        "  for sub in d_set:\n",
        "\n",
        "    # Some float values were getting through, this makes everything a string\n",
        "    if type(sub) != 'str':\n",
        "      sub = str(sub)\n",
        "    \n",
        "    # Max of split words is out max word count\n",
        "    tmp.append(sub.lower().split(maxsplit=WORD_COUNT))\n",
        "\n",
        "  tokenized_d_set.append(tmp)\n",
        "\n",
        "Tmsg0 = tokenized_d_set[0]\n",
        "Tmsg1 = tokenized_d_set[1]\n",
        "Tmsg2 = tokenized_d_set[2]\n",
        "Umsg = tokenized_d_set[3]"
      ],
      "metadata": {
        "id": "EP-G-jLIE6Oj"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now for word embedding**"
      ],
      "metadata": {
        "id": "aB9EePRHYnCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_vector(word):\n",
        "  # Checks if word is in model's vocabulary\n",
        "  if word in wv_model.wv.vocab:\n",
        "    return wv_model.wv.__getitem__(word)\n",
        "  else:\n",
        "    return np.zeros(VECTOR_DIM)\n"
      ],
      "metadata": {
        "id": "qr8NZUocgfGI"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vector_sets = []\n",
        "\n",
        "for d_set in tokenized_d_set:\n",
        "  output_d_set = []\n",
        "\n",
        "  for sequence in d_set:\n",
        "    seq_len = len(sequence)\n",
        "    output_seq = []\n",
        "\n",
        "    for word_num in range(WORD_COUNT):\n",
        "      # Zero padding for messages shorter than WORD_COUNT\n",
        "      if word_num < seq_len:\n",
        "        word = sequence[word_num]\n",
        "\n",
        "        word_vec = get_word_vector(word)\n",
        "      else:\n",
        "        word_vec = np.zeros(VECTOR_DIM)\n",
        "      \n",
        "      output_seq.append(word_vec)\n",
        "\n",
        "    output_d_set.append(output_seq)\n",
        "\n",
        "  word_vector_sets.append(output_d_set)"
      ],
      "metadata": {
        "id": "D-cNnko8YuLC"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def func_word2vec(seq):\n",
        "  seq_len = len(seq)\n",
        "  output_seq = []\n",
        "\n",
        "  for word_num in range(WORD_COUNT):\n",
        "    # Zero padding for messages shorter than WORD_COUNT\n",
        "    if word_num < seq_len:\n",
        "      word = seq[word_num]\n",
        "\n",
        "      word_vec = get_word_vector(word)\n",
        "    else:\n",
        "      word_vec = np.zeros(VECTOR_DIM)\n",
        "    \n",
        "    output_seq.append(word_vec)\n",
        "\n",
        "  return output_seq\n"
      ],
      "metadata": {
        "id": "NVyYUeJh5nC4"
      },
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for converting a sequence of word vectors back to text\n",
        "\n",
        "def vec2word(seq):\n",
        "  output_seq = []\n",
        "\n",
        "  for vec in seq:\n",
        "    word = wv_model.wv.most_similar(positive=[vec], topn=1)\n",
        "\n",
        "    # If similarity score is zero, give a blank word\n",
        "    if word[0][1] == 0.0:\n",
        "      word = [['']]\n",
        "\n",
        "    output_seq.append(word[0][0])\n",
        "\n",
        "  return output_seq"
      ],
      "metadata": {
        "id": "5Im-9wJIjOjZ"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting Data**"
      ],
      "metadata": {
        "id": "9moaIwIWsJgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Amount of items that will be fed to the model for training\n",
        "TRAINING_PERCENTAGE = 0.80\n",
        "\n",
        "training_amount = int(TRAINING_PERCENTAGE * len(word_vector_sets[0])) \n",
        "\n",
        "training_x = []\n",
        "for arr in word_vector_sets[:3]:\n",
        "  training_x.append(arr[:training_amount])\n",
        "training_x = np.array(training_x)\n",
        "training_x = np.moveaxis(training_x, 0, 1)\n",
        "\n",
        "training_y = np.array(word_vector_sets[3][:training_amount])\n",
        "\n",
        "test_x = []\n",
        "for arr in word_vector_sets[:3]:\n",
        "  test_x.append(arr[training_amount:])\n",
        "test_x = np.array(test_x)\n",
        "test_x = np.moveaxis(test_x, 0, 1)\n",
        "\n",
        "test_y = np.array(word_vector_sets[3][training_amount:])"
      ],
      "metadata": {
        "id": "zS3I6SBip3l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Defining our model**"
      ],
      "metadata": {
        "id": "IVqbPKEHwzw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Not expecting this model to perform well at all, but curious what kinds\n",
        "### Of outputs it will give\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(512, activation='relu', input_shape=(3, WORD_COUNT, VECTOR_DIM)))\n",
        "model.add(tf.keras.layers.Dense(1024, activation='relu'))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(2000, activation='relu'))\n",
        "\n",
        "model.add(tf.keras.layers.Reshape((WORD_COUNT, VECTOR_DIM)))"
      ],
      "metadata": {
        "id": "ZEZ0jiaT-f4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxWd5e_z-pZk",
        "outputId": "d55e0669-7dcb-4eda-be4e-8a4253595b18"
      },
      "execution_count": 311,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_29 (Dense)            (None, 3, 20, 512)        51712     \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 3, 20, 100)        51300     \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 6000)              0         \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 2000)              12002000  \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 20, 100)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,105,012\n",
            "Trainable params: 12,105,012\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "g7cGElMN3Wgm"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model**"
      ],
      "metadata": {
        "id": "2gyV6fN73hzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(training_x, training_y, epochs=10)  # we pass the data, labels and epochs and watch the magic!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twjTciaV3llC",
        "outputId": "f56fe648-fe93-4778-8ac8-199972de1c6d"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "80/80 [==============================] - 14s 171ms/step - loss: -18.9147 - accuracy: 0.0090\n",
            "Epoch 2/10\n",
            "80/80 [==============================] - 14s 172ms/step - loss: -26.2403 - accuracy: 0.0138\n",
            "Epoch 3/10\n",
            "80/80 [==============================] - 13s 169ms/step - loss: -28.3396 - accuracy: 0.0154\n",
            "Epoch 4/10\n",
            "80/80 [==============================] - 14s 172ms/step - loss: -30.6738 - accuracy: 0.0149\n",
            "Epoch 5/10\n",
            "80/80 [==============================] - 13s 161ms/step - loss: -34.2935 - accuracy: 0.0146\n",
            "Epoch 6/10\n",
            "80/80 [==============================] - 11s 142ms/step - loss: -35.8718 - accuracy: 0.0134\n",
            "Epoch 7/10\n",
            "80/80 [==============================] - 13s 167ms/step - loss: -35.9080 - accuracy: 0.0145\n",
            "Epoch 8/10\n",
            "80/80 [==============================] - 14s 169ms/step - loss: -39.0481 - accuracy: 0.0144\n",
            "Epoch 9/10\n",
            "80/80 [==============================] - 14s 170ms/step - loss: -44.0895 - accuracy: 0.0153\n",
            "Epoch 10/10\n",
            "80/80 [==============================] - 14s 170ms/step - loss: -48.5064 - accuracy: 0.0171\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd0996c1b90>"
            ]
          },
          "metadata": {},
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_x,  test_y, verbose=1) \n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tD2R1dWk3r5V",
        "outputId": "d14ab004-c3fd-4874-f4e9-09e3aacce63c"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20/20 [==============================] - 1s 41ms/step - loss: -13.2933 - accuracy: 0.0049\n",
            "Test accuracy: 0.004945055115967989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the model**"
      ],
      "metadata": {
        "id": "7GGVzdDe45Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_msgs = [['okay', 'so', 'first', 'iteration', 'of', 'neural', 'network', 'is', 'just', 'mid', 'stroke'],\n",
        "              ['woah', 'thats', 'really', 'sick'],\n",
        "              ['huh', 'it', 'came', 'out']]"
      ],
      "metadata": {
        "id": "7SY6cbFU5Aqn"
      },
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_vecs = []\n",
        "for msg in input_msgs:\n",
        "  input_vecs.append(func_word2vec(msg))"
      ],
      "metadata": {
        "id": "c5al1JYR5gui"
      },
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn back into strings to make sure vectors were converted correctly\n",
        "for input_msg in input_vecs:\n",
        "  print(vec2word(input_msg))"
      ],
      "metadata": {
        "id": "SJzpNvvn95Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(np.array([input_vecs]))"
      ],
      "metadata": {
        "id": "qJAP5e6x4yw-"
      },
      "execution_count": 353,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vec2word(predictions[0]))"
      ],
      "metadata": {
        "id": "wRhuaVtpBCbJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}